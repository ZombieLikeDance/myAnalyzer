# GET WHAT YOU WANT, NOT WHAT YOU DON‚ÄôT: IMAGE CONTENT SUPPRESSION FOR TEXT-TO-IMAGE DIFFUSION MODELS

Senmao $\mathbf { L i } ^ { 1 }$ , Joost van de Weijer2, Taihang $\mathbf { H } \mathbf { u } ^ { 1 }$ , Fahad Shahbaz Khan $^ { 3 , 4 }$ , Qibin Hou1   
Yaxing Wang1, Jian Yang1   
1VCIP, CS, Nankai University, 2Universitat Auto\`noma de Barcelona   
3Mohamed bin Zayed University of AI, 4Linkoping University   
senmaonk,hutaihang00 @gmail.com, joost@cvc.uab.es   
fahad.khan@liu.se, {houqb ,yaxing,csjyang}@nankai.edu.cn

# ABSTRACT

The success of recent text-to-image diffusion models is largely due to their capacity to be guided by a complex text prompt, which enables users to precisely describe the desired content. However, these models struggle to effectively suppress the generation of undesired content, which is explicitly requested to be omitted from the generated image in the prompt. In this paper, we analyze how to manipulate the text embeddings and remove unwanted content from them. We introduce two contributions, which we refer to as soft-weighted regularization and inference-time text embedding optimization. The first regularizes the text embedding matrix and effectively suppresses the undesired content. The second method aims to further suppress the unwanted content generation of the prompt, and encourages the generation of desired content. We evaluate our method quantitatively and qualitatively on extensive experiments, validating its effectiveness. Furthermore, our method is generalizability to both the pixel-space diffusion models (i.e. DeepFloyd-IF) and the latent-space diffusion models (i.e. Stable Diffusion).

# 1 INTRODUCTION

Text-based image generation aims to generate high-quality images based on a user prompt (Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2021). This prompt is used by the user to communicate the desired content, which we call the positive target, and can potentially also include undesired content, which we define with the term negative target. Negative lexemes are ubiquitously prevalent and serve as pivotal components in human discourse. They are crucial for humans to precisely communicate the desired image content.

However, existing text-to-image models can encounter challenges in effectively suppressing the generation of the negative target. For example, when requesting an image using the prompt ‚Äùa face without glasses‚Äù, the diffusion models (i.e., SD) synthesize the subject without ‚Äùglasses‚Äù, as shown in Fig. 1 (the first column). However, when using the prompt ‚Äùa man without glasses‚Äù, both SD and DeepFloyd-IF models still generate the subject with ‚Äùglasses‚Äù 1 , as shown in Fig. 1 (the second and fifth columns). Fig. 1 (the last column) quantitatively show that SD has 0.819 DetScore for ‚Äùglasses‚Äù using 1000 randomly generated images, indicating a very common failure cases in diffusion models. Also, when giving the prompt ‚Äùa man‚Äù, often the glasses are included, see Fig. 1 (the third and sixth columns). This is partially due to the fact that many of the collected man training images contain glasses, but often do not contain the glasses label (see in Appendix A Fig. 9).

Few works have addressed the aforementioned problem. The negative prompt technique2 guides a diffusion model to exclude specific elements or features from the generated image. It, however, often leads to an unexpected impact on other aspects of the image, such as changes to its structure and style (see Fig. 6). Both P2P (Hertz et al., 2022) and SEGA (Brack et al., 2023) allow steering the diffusion process along several directions, such as weakening a target object from the generated image. We empirically observe these methods to lead to inferior performance (see Fig. 6 and Table 1 below). This is expected since they are not the tailored for this problem. Recent works (Gandikota et al., 2023; Kumari et al., 2023; Zhang et al., 2023) fine-tune the SD model to eliminate completely some targeted object information, resulting in catastrophic neglect (Kumari et al., 2022). A drawback of these methods is that the model is unable to generate this context in future text-prompts. Finally, Inst-Inpaint (Yildirim et al., 2023) requires paired images to train a model to erase unwanted pixels.

![](images/cb1742607e58fb452d43a55fa20849b637e1137b0a6562774b7944afb1724af0.jpg)  
Figure 1: Failure cases of Stable Diffusion (SD) and DeepFloyd-IF. Given the prompt ‚ÄùA man without glasses‚Äù, both SD and DeepFloyd-IF fail to suppress the generation of negative target glasses. Our method successfully removes the ‚Äùglasses‚Äù. (Right) we use DetScore (see Sec. 4) to detect the ‚Äùglasses‚Äù from 1000 generated images. The DetScore of SD with prompt ‚ÄùA face without glasses‚Äù is 0.122. See Appendix E for additional examples.

In this work, we propose an alternative approach for negative target content suppression. Our method does not require fine-tuning the image generator, or collecting paired images. It consists of two main steps. In the first step, we aim to remove this information from the text embeddings3 which decide what particular visual content is generated. To suppress the negative target generation, we eliminate its information from the whole text embeddings. We construct a text embedding matrix, which consists of both the negative target and [EOT] embeddings. We then propose a soft-weighted regularization for this matrix, which explicitly suppresses the corresponding negative target information from the [EOT] embeddings. In the second step, to further improve results, we apply inference-time text embedding optimization which consists of optimizing the whole embeddings (processed in the first step) with respect to two losses. The first loss, called negative target prompt suppression, weakens the attention maps of the negative target further suppressing negative target generation. This may lead to the unexpected suppression of the positive target (see Appendix D. Fig. 13 (the third row)). To overcome this, we propose a positive target prompt preservation loss that strengthens the attention maps of the positive target. Finally, the combination of our proposed regularization of the text embedding matrix and the inference-time embedding optimization leads to improved negative target content removal during image generation.

In summary, our work makes the following contributions: (I) Our analysis shows that the [EOT] embeddings contain significant, redundant and duplicated semantic information of the whole input prompt (the whole embeddings). This needs to be taken into account when removing negative target information. Therefore, we propose soft-weighted regularization to eliminate the negative target information from the [EOT] embeddings. (II) To further suppress the negative target generation, and encourage the positive target content, we propose inference-time text embedding optimization. Ablation results confirm that this step significantly improves final results. (III) Through extensive experiments, we show the effectiveness of our method to correctly remove the negative target information without detrimental effects on the generation of the positive target content. Our code is available in https://github.com/sen-mao/SuppressEOT.

# 2 RELATED WORK

Text-to-Image Generation. Text-to-image synthesis aims to synthesize highly realistic images which are semantically consistent with the text descriptions. More recently, text-to-image models (Saharia et al., 2022; Ramesh et al., 2022; Rombach et al., 2021) have obtained amazing performance in image generation. With powerful image generation capability, diffusion models allow users to provide a text prompt, and generate images of unprecedented quality. Furthermore, a series of recent works investigated knowledge transfer on diffusion models (Kawar et al., 2022; Ruiz et al., 2022; Valevski et al., 2022; Kumari et al., 2022) with one or few images. In this paper, we focus on the Stable Diffusion (SD) model without fientuning, and address the failure case that the generated subjects are not corresponding to the input text prompts.

Diffusion-Based Image Generation. Most recent works explore the ability to control or edit a generated image with extra conditional information, as well as text information. It contains label-toimage generation, layout-to-image generation and (reference) image-to-image generation. Specifically, label-to-image translation (Avrahami et al., 2022a;b; Nichol et al., 2021) aims to synthesize high-realistic images conditioning on semantic segmentation information, as well as text information. P2P (Hertz et al., 2022) proposes a mask-free editing method. Similar to label-to-image translation, both layout-to-image (Li et al., 2023b; Zhang & Agrawala, 2023) and (reference) image-toimage (Brooks et al., 2022; Parmar et al., 2023) generations aim to learn a mapping from the input image map to the output image. GLIGEN(Li et al., 2023b) boosts the controllability of the generated image by inserting bounding boxes with object categories. Some works investigate Diffusionbased inversion. (Dhariwal & Nichol, 2021) shows that a given real image can be reconstructed by DDIM (Song et al., 2020) sampling. Recent works investigate either the text embeddings of the conditional input (Gal et al., 2022; Li et al., 2023a; Wang et al., 2023), or the null-text optimization of the unconditional input(i.e., Null-Text Inversion (Mokady et al., 2022)).

Diffusion-Based Semantic Erasion. Current approaches (Gandikota et al., 2023; Kumari et al., 2023; Zhang et al., 2023) have noted the importance of erasure, including the erasure of copyright, artistic style, nudity, etc. ESD (Gandikota et al., 2023) utilizes negative guidance to lead the fine-tuning of a pre-trained model, aiming to achieve a model that erases specific styles or objects. (Kumari et al., 2023) fine-tunes the model using two prompts with and without erasure terms, such that the model distribution matches the erasure prompt. Inst-Inpaint (Yildirim et al., 2023) is a novel inpainting framework that trains a diffusion model to map source images to target images with the inclusion of conditional text prompts. However, these works fine-tune the SD model, resulting in catastrophic neglect for the unexpected suppression from input prompt. In this paper, we aim to remove unwanted subjects in output images without further training or fine-tuning the SD model.

# 3 METHOD

We aim to suppress the negative target generation in diffusion models. To achieve this goal, we focus on manipulating the text embeddings, which essentially control the subject generation. Naively eliminating a target text embedding fails to exclude the corresponding object from the output (Fig. 2a (the second and third columns)). We conduct a comprehensive analysis that shows this failure is caused by the appended [EOT] embeddings (see Sec. 3.2). Our method consists of two main steps. In the first step, we propose soft-weighted regularization to largely reduce the negative target text information from the [EOT] embeddings (Sec. 3.3). In the second step, we apply inference-time text embedding optimization which consists of optimizing the whole text embeddings (processed in the first step) with respect to two losses. The first loss, called the negative target prompt suppression loss, aims to weaken the attention map of the negative target to guide the update of the whole text embeddings, thus further suppressing the subject generation of the negative target. To prevent undesired side effects, namely the unexpected suppression from the positive target in the output (see Appendix D. Fig. 13 (the third row)), we propose the positive target prompt preservation loss. This strengthens the attention map of the positive target. The inference-time text embedding optimization is presented in Sec. 3.4. In Sec. 3.1, we provide a simple introduction to the SD model, although our method is not limited to a specific diffusion model.

# 3.1 PRELIMINARY: DIFFUSION MODEL

The SD firstly trains an encoder $E$ and a decoder $D$ . The encoder maps the image $\scriptstyle { \mathbf { { \vec { x } } } }$ into the latent representation $z _ { 0 } = E ( { \pmb x } )$ , and the decoder reverses the latent representation $z _ { 0 }$ into the image

![](images/ce7f3ca8eda1f36280bd2cab34598869690a42c2843cd210cc26aed903b447f8.jpg)  
Figure 2: Analysis of [EOT] embeddings. (a) [EOT] embeddings contain significant information as can be seen when zeroed out. (b) when performing WNNM (Gu et al., 2014), we find that [EOT] embeddings have redundant semantic information. (c) distance matrix between all text embeddings. Note that each [EOT] embedding contains similar semantic information and they have near zero distance.

$\hat { \pmb x } = D ( z _ { \mathbf { 0 } } )$ . SD trains a UNet-based denoiser network $\epsilon _ { \theta }$ to predict noise $\epsilon$ , following the objective:

$$
\operatorname* { m i n } _ { \theta } E _ { z _ { 0 } , \epsilon \sim N ( 0 , I ) , t \sim \left[ 1 , T \right] } \left\| \epsilon - \epsilon _ { \theta } ( z _ { t } , t , c ) \right\| _ { 2 } ^ { 2 } ,
$$

where the encoded text embeddings $c$ is extracted by a pre-trained CLIP text encoder $\Gamma$ with given a conditioning prompt $\pmb { p }$ : $\pmb { c } = \Gamma ( \pmb { p } )$ , $\scriptstyle \boldsymbol { z } _ { t }$ is a noise sample at timestamp $t \sim [ 1 , T ]$ , and $T$ is the number of the timestep. The SD model introduces the cross-attention layer by incorporating the prompt. We could extract the internal cross-attention maps $A$ , which are high-dimensional tensors that bind pixels and tokens extracted from the prompt text.

# 3.2 ANALYSIS OF [EOT] EMBEDDINGS

The text encoder $\Gamma$ maps input prompt $\pmb { p }$ into text embeddings $\pmb { c } = \Gamma ( \pmb { p } ) \in \mathbb { R } ^ { M \times N }$ (i.e., $M =$ 768, $N = 7 7$ in the SD model). This works by prepending a Start of Text ([SOT]) symbol to the input prompt $\pmb { p }$ and appending $N - | p | - 1$ End of Text ([EOT]) padding symbols at the end, to obtain $N$ symbols in total. We define‚àíte|xt e‚àímbeddings $\pmb { c } = \{ \pmb { c } ^ { S O \tilde { T } } , \pmb { c } _ { 0 } ^ { P } , \cdots , \pmb { c } _ { | p | - 1 } ^ { P } , \pmb { c } _ { 0 } ^ { E O T } , \cdots , \pmb { c } _ { N - | p | - 2 } ^ { E O T } \}$ Below, we explore several aspects of the [EOT] embeddings.

What semantic information [EOT] embeddings contain? We observe that [EOT] embeddings carry significant semantic information. For example, when requesting an image with the prompt ‚Äùa man without glasses‚Äù, SD synthesizes the subject including the negative target ‚Äùglasses‚Äù (Fig. 2a (the first column)). When zeroing out the token embedding of ‚Äùglasses‚Äù from the text embeddings $\scriptstyle { c }$ , SD fails to discard ‚Äùglasses‚Äù (Fig. 2a (the second and third columns)). Similarly, zeroing out all [EOT] embeddings still generates the ‚Äùglasses‚Äù subject (Fig. 2a (the fourth and fifth columns)). Finally, when zeroing out both ‚Äùglasses‚Äù and the [EOT] token embeddings, we successfully remove ‚Äùglasses‚Äù from the generated image (Fig. 2a (the sixth and seventh columns)). The results suggest that the [EOT] embeddings contain significant information about the input prompt. Note that naively zeroing them out often leads to unexpected changes (Fig. 2a (the seventh column)).

How much information whole [EOT] embeddings contain? We experimentally observe that [EOT] embeddings have the low-rank property 4, indicating they contain redundant semantic information. The weighted nuclear norm minimization (WNNM) (Gu et al., 2014) is an effective

cP(i = 0,1,., IPl|- 1)   
CSOT CPE CNE EOT EOT CSOT cPE CNEEOT. EOT N-|P|-2   
100 011. Soft-weighted 011fl. regularization   
CNE CEOT. EOT N-|P|-2 ¬£ ¬£ CNE CEOT.. ^EOT N-|pl ‰∏≠ T APE ANE   
‰∏â U¬£VT Eq.2 UEVT Eq.3 Eq.4 X X APE ANE (a) Soft-weighted regularization (b) Inference-time optimization

lwoewc-roanskt uacntalay[sEisOmT]etehmodb. dWienglsevmeratrgiex $\Psi = [ \pmb { c } _ { 0 } ^ { E O T } , \pmb { c } _ { 1 } ^ { E O T } , \cdot \cdot \cdot \ \overline { { } } \mathbf { , } \pmb { c } _ { N - \mid p \mid - 2 } ^ { E O T } ]$ ,eadndidnpgesr. Srpme ciWfiNcaNllMy, as follows $\mathcal { D } _ { w } ( \Psi ) = U \mathcal { D } _ { w } ( \Sigma ) V ^ { T }$ , where $\Psi = \pmb { U } \pmb { \Sigma } \pmb { V } ^ { T }$ is the Single Value Decomposition (SVD) of $\Psi$ , and ${ \mathcal { D } } _ { w } ( { \pmb { \Sigma } } )$ is the generalized soft-thresholding operator with the weighted vector ${ \pmb w }$ , i.e., $\mathcal { D } _ { w } \big ( \Sigma \big ) _ { i i } = \mathrm { s o f t } \big ( \Sigma _ { i i } , w _ { i } \big ) = \mathrm { m a x } \big ( \Sigma _ { i i } - w _ { i } , 0 \big )$ . The singular values $\pmb { \sigma } _ { 0 } \geq \dots \geq \pmb { \sigma } _ { N - | \pmb { p } | - 2 }$ and the weights satisfy $0 \leq w _ { 0 } \leq \cdot \cdot \cdot \leq w _ { N - | p | - 2 }$ .

To verify the low-rank property of [EOT] embeddings, WNNM mainly keeps the top- $\mathbf { \nabla } \cdot K$ largest singular values of $\pmb { \Sigma }$ , zero out the small singular values, and finally reconstruct $\hat { \Psi } \ =$ $\left[ \hat { \pmb { c } } _ { 0 } ^ { E O T } , \hat { \pmb { c } } _ { 1 } ^ { E O T } , \cdot \cdot \cdot , \hat { \pmb { c } } _ { N - | \pmb { p } | - 2 } ^ { E O T } \right]$ . We use $\mathrm { R a n k } ( \hat { \Psi } )$ to represent the rank of $\hat { \Psi }$ . We explore the impact of different $\mathrm { R a n k } ( \hat { \Psi } )$ values on the generated image. For example, as shown in Fig. 2b, with the prompt ‚ÄùWhite and black long coated puppy‚Äù (here $| p | = 6 )$ , we use PSNR and SSIM metrics to evaluate the modified image against the SD model‚Äôs output. Setting $\operatorname { R a n k } ( { \hat { \Psi } } ) { = } 0$ , zeroing all [EOT] embeddings, the generated image preserves similar semantic information as when using all [EOT] embeddings. As $\mathbf { \hat { R a n k } } ( \hat { \Psi } )$ increases, the generated image gets closer to the SD model‚Äôs output. Visually, the generated image looks similar to the one of the SD model with $\mathrm { R a n k } ( \hat { \Psi } ) { = } 4$ . Achieving acceptable metric values $\mathrm { \Delta } ^ { \prime } \mathrm { P S N R } { = } 4 0 . 2 8 8$ , $\mathrm { S S I M { = } 0 . 9 9 4 } ^ { \cdot }$ ) with $\operatorname { R a n k } ( { \hat { \Psi } } ) { = } 9$ in Fig. 2b (middle). The results indicate that the [EOT] embeddings have the low-rank property, and contain redundant semantic information.

Semantic alignment for each [EOT] embedding There exist a total of $7 6 - | p |$ [EOT] embeddings. However, we find that the various [EOT] embeddings are highly correlated, and they typically contain the semantic information of the input prompt. This phenomenon is demonstrated both qualitatively and quantitatively in Fig. 2c. For example, we input the prompt ‚ÄùA man with a beard wearing glasses and a beanie in a blue shirt‚Äù. We randomly select one [EOT] embedding to replace input text embeddings like Fig. 2c (left) 5. The generated images have similar semantic information (Fig. 2c (right)). This conclusion also is demonstrated by the distance of each [EOT] embedding (Fig. 2c (middle)). Most [EOT] embeddings have small distance among themselves. In conclusion, we need to remove the negative target information from the $7 6 - | p |$ [EOT] embeddings.

# 3.3 TEXT EMBEDDING-BASED SEMANTIC SUPPRESSION

Our goal is to suppress negative target information during image generation. Based on the aforementioned analysis, we must eliminate the negative target information from the [EOT] embeddings. To achieve this goal, we introduce two strategies, which we refer to as soft-weighted regularization and inference-time text embedding optimization. For the former, we devise a negative target embedding matrix, and propose a new method to regularize the negative target information. The inference-time text embedding optimization aims to further suppress the negative target generation of the target prompt, and encourages the generation of the positive target. We give an overview of the two strategies in Fig. 3.

Soft-weighted Regularization. We propose to use Single Value Decomposition (SVD) to extract negative target information (e.g., glasses) from the text embeddings. Let $\textbf { \textit { c } } =$ $\{ \pmb { c } ^ { S O T } , \pmb { c } _ { 0 } ^ { P } , \cdots , \pmb { c } _ { | \pmb { p } | - 1 } ^ { P } , \pmb { c } _ { 0 } ^ { E O T } , \cdots , \pmb { c } _ { N - | \pmb { p } | - 2 } ^ { E O T } \}$ , cENOTp 2} be the text embeddings from CLIP text encoder. As shown in Fig. 3 (left), we split the embeddings $\pmb { c } _ { i } ^ { P } ( i \ = \ 0 , 1 , \cdots , | \pmb { p } | \textrm { - } 1 )$ into the negac $c ^ { N E }$ $c ^ { P E }$ $\begin{array} { r } { \mathrm { : } = \{ c ^ { \check { S } O T } , c _ { 0 } ^ { P } , \cdots , \check { c } _ { | \mathcal { p } | - 1 } ^ { P } , c _ { 0 } ^ { E O T } , \cdots , c _ { N - | \mathcal { p } | - 2 } ^ { \check { S } O T } \} = \check { \{ } } c ^ { S O T } , c ^ { P E } , c ^ { N E } , c _ { 0 } ^ { E O T } , \cdots , c _ { N - | \mathcal { p } | - 2 } ^ { E O T } \} .  \end{array}$ We construct a negative target embedding matrix $x$ : $\pmb { \chi } \ = \ \left[ \pmb { c } ^ { N E } , \pmb { c } _ { 0 } ^ { E O T } , \cdot \cdot \cdot \ , \pmb { c } _ { N - | p | - 2 } ^ { E O T } \right]$ We perform SVD: ${ \boldsymbol { x } } ~ = ~ { \boldsymbol { U } } { \boldsymbol { \Sigma } } { \boldsymbol { V } } ^ { T }$ , where $\Sigma ~ = ~ d i a g ( { \sigma } _ { 0 } , { \sigma } _ { 1 } , \cdot \cdot \cdot , { \sigma } _ { n _ { 0 } } )$ , the singular values $\sigma _ { 1 } ~ \geq$ $\cdots \geq \sigma _ { n _ { 0 } }$ , $n _ { 0 } ~ = ~ \mathrm { m i n } ( M , N - | \pmb { p } | - 1 )$ . Intuitively, the negative target embedding matrix $\pmb { \chi } = \left[ \pmb { c } ^ { N E } , \pmb { c } _ { 0 } ^ { E O T } , \cdot \cdot \cdot , \pmb { c } _ { N - | \pmb { p } | - 2 } ^ { E O T } \right]$ mainly contains the expected suppressed information. After performing SVD, we assume that the main singular values are corresponding to the suppressed information (the negative target). Then, to suppress negative target information, we introduce softweighted regularization for each singular value 6:

$$
\hat { \sigma } = e ^ { - \sigma } * \sigma .
$$

We then recover the embedding matrix $\hat { \pmb { \chi } } = \pmb { U } \hat { \pmb { \Sigma } } \pmb { V } ^ { T }$ , here $\hat { \pmb { \Sigma } } = d i a g ( \hat { \sigma _ { 0 } } , \hat { \sigma _ { 1 } } , \cdot \cdot \cdot , \hat { \sigma _ { n _ { 0 } } } )$ . Note that the recovered structure is œáÀÜ = hcÀÜNE, cÀÜ0EOT , ¬∑ $\hat { x } = \left[ \hat { c } ^ { N E } , \hat { c } _ { 0 } ^ { E O T } , \cdot \cdot \cdot , \hat { c } _ { N - \mid p \mid - 2 } ^ { E O T } \right]$ cÀÜENOTp 2i, and cÀÜ = {cSOT , cP E, cÀÜNE, cÀÜ0EOT , $\hat { c } _ { N - | p | - 2 } ^ { E O T } \}$

We consider a special case where we reset top- $\mathbf { K }$ or bottom- $\mathbf { \nabla } \cdot \mathbf { K }$ singular values to 0. As shown on Fig. 4, we are able to remove the negative target prompt (e.g, glasses or beard) when setting the top-K (here, ${ \mathrm { K } } = 2$ ) singular values to 0. And the negative target prompt information is preserved when the bottom-K singular values are set to 0 (here, $\mathrm { K } { = } 7 0$ ). This supports our assumption that main singular values of $x$ are corresponding to the negative target information.

# 3.4 INFERENCE-TIME TEXT EMBEDDING OPTIMIZATION

As illustrated in Fig. 3 (right), for a specific timestep $t$ , during the diffusion process $T \to 1$ , we get twhheedrief $\epsilon _ { \theta } ( \widetilde { z } _ { t } , t , c )$ .rrTeshpeoanttdeintgioanttemnatipos sa: $( A _ { t } ^ { P E } , A _ { t } ^ { N \tilde { E } } )$ , $\pmb { c } = \{ \pmb { c } ^ { S O T } , \pmb { c } ^ { P E } , \pmb { \dot { c } ^ { N E } } , \pmb { c } _ { 0 } ^ { E O T } , \cdots , \pmb { c } _ { N - | p | - 2 } ^ { E O T } \}$ $A _ { t } ^ { P \dot { E } }$ to $c ^ { P E }$ , while $A _ { t } ^ { N E }$ are corresponding to $\pmb { c } ^ { N E }$ which we aim to suppress. After soft-weighted regularization, we have the new text embeddings cÀÜ = {cSOT , cP E, cÀÜNE, cÀÜ0EOT , . Similarly, we are able to get the attention maps: $( \hat { A } _ { t } ^ { P E } , \hat { A } _ { t } ^ { N E } )$ .

Here, we aim to further suppress the negative target generation, and encourage the positive target information. We propose two attention losses to regularize the attention maps, and modify the text embeddings $\hat { \ b { c } }$ to guide the attention maps to focus on the particular region, which is corresponding to the positive target prompt. We introduce an positive target prompt preservation loss:

$$
\mathcal { L } _ { p l } = \left. \hat { A } _ { t } ^ { P E } - A _ { t } ^ { P E } \right. ^ { 2 } .
$$

That is, the loss attempts to strengthen the attention maps of the positive target prompt at the timestep $t$ . To further suppress generation for the negative target prompt, we propose the negative target prompt suppression loss:

$$
\mathcal { L } _ { n l } = - \left. \hat { A } _ { t } ^ { N E } - A _ { t } ^ { N E } \right. ^ { 2 } ,
$$

Full objective. The full objective function of our model is:

$$
\begin{array} { r } { \mathcal { L } = \lambda _ { p l } \mathcal { L } _ { p l } + \lambda _ { n l } \mathcal { L } _ { n l } , } \end{array}
$$

Table 1: Comparison with baselines. The best results are in bold, and the second best results are underlined.   

<html><body><table><tr><td rowspan="3">Method</td><td colspan="4">Real-image editing</td><td colspan="8">Generated-image editing</td></tr><tr><td colspan="2">Random negative target</td><td colspan="4">Random negative target</td><td colspan="2">Negative target: Car</td><td colspan="2">Negytive ainet:</td><td colspan="2"> Negativeoghet:</td></tr><tr><td>Clipscore‚ÜìIFID‚ÜëDetScore‚ÜìClipscore‚ÜìIFID‚Üë DetScore‚ÜìClipscore‚ÜìIFID‚ÜëDetScore‚ÜìClipscoreIFID‚ÜëClipscoreIFID‚Üë</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Real image or SD (Generated image)</td><td>0.7986</td><td>0</td><td>0.3381</td><td>0.8225</td><td>0</td><td>0.4509</td><td>0.8654</td><td>0</td><td>0.6643</td><td>0.7414</td><td>0</td><td>0.8770 0</td></tr><tr><td>Negative prompt</td><td>0.7983</td><td>175.8</td><td>0.2402</td><td>0.7619</td><td>169.0</td><td>0.1408</td><td>0.8458</td><td>151.7 0.5130</td><td></td><td>0.7437</td><td>233.9</td><td>0.8039 242.1</td></tr><tr><td>P2P (Hertz et al., 2022)</td><td>0.7666</td><td>92.53</td><td>0.1758</td><td>0.8118</td><td>103.3</td><td>0.3391</td><td>0.8638 21.7</td><td>0.6343</td><td>0.7470</td><td>86.3</td><td>0.8849</td><td>139.7</td></tr><tr><td>ESD (Gandikota et al., 2023)</td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.7986 165.7</td><td>0.2223</td><td>0.6954</td><td>256.5</td><td>0.7292</td><td>267.5</td></tr><tr><td>Concept-ablation (Kumari et al., 2023)</td><td></td><td>Ôºö</td><td></td><td>Ôºö</td><td></td><td></td><td>0.7642 179.3</td><td>0.0935</td><td>0.7411</td><td>211.4</td><td>0.8290</td><td>219.9</td></tr><tr><td>Forget-Me-Not (Zhang et al.,2023)</td><td></td><td></td><td></td><td>Ôºö</td><td></td><td></td><td>0.8701 158.7</td><td>0.5867</td><td>0.7495</td><td>227.9</td><td>0.8391</td><td>203.5</td></tr><tr><td>Inst-Inpaint (Yildirim et al., 2023)</td><td>0.7327</td><td>135.5</td><td>0.1125</td><td>0.7602</td><td>150.4</td><td>Ôºå 0.1744</td><td>0.8009 126.9</td><td>0.2361</td><td></td><td></td><td></td><td></td></tr><tr><td>SEGA (Brack et al., 2023)</td><td></td><td></td><td></td><td>0.7960</td><td>172.2</td><td>0.3005</td><td>0.8001 168.8</td><td>0.4767</td><td>0.7678</td><td>209.9</td><td>0.8730</td><td>175.0</td></tr><tr><td>Ours</td><td>0.6857</td><td>166.3</td><td>0.0384</td><td>0.6647</td><td>176.4</td><td>0.1321</td><td>0.7426 206.8</td><td>0.0419</td><td>0.7402</td><td>217.7</td><td>0.6448</td><td>307.5</td></tr></table></body></html>

where $\lambda _ { p l } { = } 1$ and $\lambda _ { n l } { = } 0 . 5$ are used to balance the effect of preservation and suppression. We use this loss to update the text embeddings $\hat { \pmb { c } }$ .

For real image editing, we first utilize the text embeddings $c$ to apply Null-Text (Mokady et al., 2022) to invert a given real image into the latent representation. Then we use the proposed softweighted regularization to suppress negative target information from $c$ resulting in $\hat { \pmb { c } }$ . Next, we apply inference-time text embedding optimization to update $\hat { \pmb { c } } _ { t }$ during inference, resulting in the final edited image. Our full algorithm is presented in Algorithm 1. See Appendix C for more detail about negative target generation of SD model without the reference real image.

# Algorithm 1: Our algorithm

<html><body><table><tr><td>Input:A text embeddings c=I(p) and real image I. Output:Edited image I.</td></tr><tr><td>T =Inversion(E(I),c)Ôºõ// e.g.ÔºåNull-text ‚ÜêSWR(c)(Eq.2); //SWR fort=T,T-1...,1do ct=c; //ITO forite=0,...,9 do ‚Üê‚ààŒ∏(‚â•t,t,cÔºâÔºõ APE, ‚Üê‚ààŒ∏(‚â•t,t,ctÔºâÔºõ L‚ÜêXplLpl+ŒªnlLnùëñ(Eqs.3-6); ct‚Üêct-natLÔºõ end 2t-1,-,-‚Üê‚ààŒ∏(‚â•t,t,CtÔºâ APE ANE ANE</td></tr></table></body></html>

![](images/cb8c13597cf5c6efa4b81b6801586d426a4fd415bf9f1593501ede6ce7dad58d.jpg)  
Figure 4: Effect of resetting top- $K$ or bottom- $K$ singular values to 0. Main singular values correspond to the target information that we expect to be suppressed.

# 4 EXPERIMENTS

Baseline Implementations. We compare with the following baselines: Negative prompt, ESD (Gandikota et al., 2023), Concept-ablation (Kumari et al., 2023), Forget-Me-Not (Zhang et al., 2023), Inst-Inpaint (Yildirim et al., 2023) and SEGA (Brack et al., 2023). We use P2P (Hertz et al., 2022) with Attention Re-weighting.

Evaluation datasets. We evaluate the proposed method from two perspectives: generated-image editing and real-image editing. In the former, we suppress the negative target generation from a generated image of the SD model with a text prompt, and the latter refers to editing a real-image input and a text input. Similar to recent editing-related works (Mokady et al., 2022; Gandikota et al., 2023; Patashnik et al., 2023), we use nearly 100 images for evaluation. For generated-image negative target surrpression, we randomly select 100 captions provided in the COCO‚Äôs validation set (Chen et al., 2015) as prompts. The Tyler Edlin and Van Gogh related data (prompts and seeds) are obtained from the official code of ESD (Gandikota et al., 2023). For real-image negative target suppression, we randomly select 100 images and their corresponding prompts from the Unsplash 7 and COCO datasets. We also evaluate our approach on the GQA-Inpaint dataset, which contains 18,883 unique source-target-prompt pairs for testing. See Appendix. A for more details on experiments involving this dataset. We show the optimization details and more results in Appendix A, D and E, respectively.

![](images/f788e95a2008cb12af78daf332ba365dd3aad573690f406a71f733c03b67502e.jpg)  
Figure 5: (Left) We detect the negative target from the edited images and and show the DetScore below. (Middle) Real image negative target suppression results. Inst-Inpaint fills the erased area with unrealistic pixels (the red dotted line frame). Our method exploits surrounding content information. (Right) User study.

![](images/27f6eaf14ec850a0016e9725b8e6fcae337373c2b9c2c7a107912554e8737ef5.jpg)  
Figure 6: Real image (Left) and generated image (Middle and Right) negative target suppression results. (Middle) We are able to suppress the negative target, without further finetuning the SD model. (Right) Examples of negative target suppression.

Metrics. Clipscore (Hessel et al., 2021) is a metric that evaluates the quality of a pair of a negative prompt and an edited image. We also employ the widely used Fre¬¥chet Inception Distance (FID) (Heusel et al., 2017) for evaluation. To evaluate the suppression of the target prompt information after editing, we use inverted FID (IFID), which measures the similarity between two sets. In this metric, the larger the better. We also propose to use the DetScore metric, which is based on MMDetection (Chen et al., 2019) with GLIP (Li et al., 2022). We detect the negative target object in the edited image, successful editing should lead to a low DetScore (see Fig. 5 and Appendix A for more detail). Following Inst-Inpaint (Yildirim et al., 2023), we use FID and CLIP Accuracy to evaluate the accuracy of the removal operation on the GQA-Inpaint dataset.

For real-image negative target suppression, as reported in Table 1 we achieve the best score in both Clipscore and DetScore (Table 3 (the second and fourth columns)), and a comparable result in IFID.

![](images/82b10153b9d8ff30e592049c5c77603aadc6a8fdb7286af059cf79b93f274d71.jpg)  
Figure 7: Additional applications. Our method can be applied to image restoration tasks, such as shadow, cracks, and rain removal. Also we can strengthen the object generation (6-9 column).

Negative prompt has the best performance in IFID score. However, it often changes the structure and style of the image (Fig. 6 (left, the second row)). In contrast, our method achieves a better balance between preservation and suppression (Fig. 6 (left, the last row)). For generated image negative target suppression, we have the best performance for both a random and specific negative target, except for removing Tyler Edlin‚Äôs style, for which ESD obtains the best scores. However, ESD requires to finetune the SD model, resulting in catastrophic neglect. Our advantage is further substantiated by visualized results (Fig. 6).

As shown in Fig. 5 (middle) and Table 2, we achieve superior suppression results and higher CLIP Accuracy scores on the GQA-Inpaint dataset. Inst-Inpaint achieves the best FID score (Table 2 (the third column)) primarily because its results (Fig. 5 (the second row, the sixth column)) closely resemble the ground truth (GT). However, the GT images contain unrealistic pixels. Our method yields more photo-realistic results. These results demonstrate that the proposed method is effective in suppressing the negative target. See Appendix. A for more experimental details.

User study. As shown in Fig. 5 (Right), we conduct a user study. We require users to select the figure in which the negative target is more accurately suppressed. We performed septuplets comparisons (forced choice) with 20 users (20 quadruplets/user). The results demonstrate that our method outperforms other methods. See Appendix. E for more details.

Ablation analysis. We conduct an ablation study for the proposed approach. We report the quantitative result in Table 3. Using soft-weighted regularization (SWR) alone cannot completely remove objects from the image. The results indicate that using both SWR and inference-time text embedding optimization leads to the best scores. The visualized results are presented in Appendix. D.

Additional applications. As shown in Fig. 7 (the first to the fifth columns), we perform experiments on a variety of image restoration tasks, including shadow removal, cracks removal and rain removal. Interestingly, our method can also be used to remove these undesired image artifacts. Instead of extracting the negative target embedding, we can also strengthen the added prompt and [EOT] embeddings. As shown in Fig. 7 (the sixth to the ninth columns), our method can be successfully adapted to strengthen image content, and obtain results that are similar to methods like GLIGEN (Li et al., 2023b) and Attend-and-Excite (Chefer et al., 2023) (See Appendix. F for a complete explanation and more results).

<html><body><table><tr><td>Methods</td><td>Paired data</td><td>FID‚Üì</td><td>CLIP Acc ‚Üë</td><td>CLIP Acc (top5) ‚Üë</td></tr><tr><td>X-Decoder</td><td>‚àö</td><td>6.86</td><td>69.9</td><td>46.5</td></tr><tr><td>Inst-Inpaint</td><td>‚àö</td><td>5.50</td><td>80.5</td><td>60.4</td></tr><tr><td>Ours</td><td>X</td><td>13.87</td><td>92.8</td><td>83.3</td></tr></table></body></html>

able 2: Quantitative comparison on the GQA- Table 3: Ablation study. The effectivenpaint dataset for real image negative target sup- ness of both soft-weighted regularization and pression task. inference-time text embedding optimization.   

<html><body><table><tr><td></td><td>Clipscore‚Üì</td><td>IFID‚Üë</td><td>DetScore‚Üì</td></tr><tr><td>SD</td><td>0.8225</td><td>0</td><td>0.4509</td></tr><tr><td>SWR</td><td>0.7996</td><td>85.9</td><td>0.3668</td></tr><tr><td>SWR+ Lpl</td><td>0.8015</td><td>100.2</td><td>0.3331</td></tr><tr><td>SWR+Lpl+Lnl</td><td>0.6647</td><td>176.4</td><td>0.1321</td></tr></table></body></html>

# 5 CONCLUSIONS AND LIMITATIONS

We observe that diffusion models often fail to suppress the generation of negative target information in the input prompt. We explore the corresponding text embeddings and find that [EOT] embeddings contain significant, redundant and duplicated semantic information. To suppress the generation of negative target information, we provide two contributions: soft-weighted regularization and inference-time text embedding optimization. In the former, we suppress the negative target information from the text embedding matrix. The inference-time text embedding optimization encourages the postive target to be preserved, as well as further removing the negative target information. Limitations: Currently, the test-time optimization costs around half a minute making the proposed method unfit for applications that require fast results. But, we believe that a dedicated engineering effort can cut down this time significantly.

# ACKNOWLEDGEMENTS

This work was supported by funding by projects TED2021-132513B-I00 and PID2022-143257NBI00 funded by MCIN/AEI/ 10.13039/501100011033 and by the European Union NextGenerationEU/PRTR and FEDER. Computation is supported by the Supercomputing Center of Nankai University.

# REFERENCES

Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. arXiv preprint arXiv:2206.02779, 2022a.   
Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18208‚Äì18218, 2022b.   
Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, and Kristian Kersting. Sega: Instructing diffusion using semantic dimensions. arXiv preprint arXiv:2301.12247, 2023.   
Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022.   
Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-andexcite: Attention-based semantic guidance for text-to-image diffusion models. arXiv preprint arXiv:2301.13826, 2023.   
Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.   
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dolla¬¥r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.   
Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780‚Äì8794, 2021.   
Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022.   
Rohit Gandikota, Joanna Materzyn¬¥ska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts from diffusion models. arXiv preprint arXiv:2303.07345, 2023.   
Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. Weighted nuclear norm minimization with application to image denoising. In Proceedings of the IEEE conference on computer vision

and pattern recognition, pp. 2862‚Äì2869, 2014.

Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng Song, Mengwei Ren, Ruijiang Gao, Yuxiao Chen, Di Liu, Qilong Zhangli, et al. Improving negative-prompt inversion via proximal guidance. arXiv preprint arXiv:2306.05414, 2023.   
Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022.   
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: a reference-free evaluation metric for image captioning. In EMNLP, 2021.   
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. pp. 6626‚Äì 6637, 2017.   
Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Hui-Tang Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. ArXiv, abs/2210.09276, 2022.   
Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022.   
Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in text-to-image diffusion models. arXiv preprint arXiv:2303.13516, 2023.   
Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10965‚Äì10975, 2022.   
Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz Khan, Qibin Hou, Yaxing Wang, and Jian Yang. Stylediffusion: Prompt-embedding inversion for text-based editing. arXiv preprint arXiv:2303.15649, 2023a.   
Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. arXiv preprint arXiv:2301.07093, 2023b.   
Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021.   
Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models. arXiv preprint arXiv:2305.16807, 2023.   
Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022.   
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.   
Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. arXiv preprint arXiv:2302.03027, 2023.   
Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, and Daniel Cohen-Or. Localizing object-level shape variations with text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.   
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.   
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¬®rn Ommer. Highresolution image synthesis with latent diffusion models, 2021.   
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022.   
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic textto-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.   
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020.   
Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1921‚Äì1930, 2023.   
Dani Valevski, Matan Kalman, Yossi Matias, and Yaniv Leviathan. Unitune: Text-driven image editing by fine tuning an image generation model on a single image. arXiv preprint arXiv:2210.09477, 2022.   
Kai Wang, Fei Yang, Shiqi Yang, Muhammad Atif Butt, and Joost van de Weijer. Dynamic prompt learning: Addressing cross-attention leakage for text-based image editing. In Proc. NeurIPS, 2023.   
Ahmet Burak Yildirim, Vedat Baday, Erkut Erdem, Aykut Erdem, and Aysegul Dundar. Inst-inpaint: Instructing to remove objects with diffusion models, 2023.   
Yu Zeng, Zhe Lin, Huchuan Lu, and Vishal M Patel. Cr-fill: Generative image inpainting with auxiliary contextual reconstruction. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 14164‚Äì14173, 2021.   
Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi. Forget-me-not: Learning to forget in text-to-image diffusion models. arXiv preprint arXiv:2211.08332, 2023.   
Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models,

2023.

# A APPENDIX: IMPLEMENTATION DETAILS

Configure. We suppress semantic information by optimizing whole text embeddings at inference time, and it takes as little as 35 seconds. No extra network parameters are required in our optimization process. We mainly use the Stable Diffusion v1.4 pre-trained model 8. All of our experiments are conducted using a Quadro RTX 3090 GPU (24GB VRAM).

Early stop. Recent works Hertz et al. (2022); Chefer et al. (2023) demonstrate that the spatial location of each subject is decided in the early step. Thus we validate our method on different steps in inference time. Fig 8 (left) shows that our method suffers from artifacts after 20 timesteps. In this paper, at inference time we apply the proposed method among $0  2 0$ timesteps, and for the remaining timesteps, we perform the original image generation as done in the SD model.

Inner iterations. Fig 8 (right) shows the generation at different iterations within each timestep. We observe that the output images undergo unexpected change after 10 iterations. We set the iteration number to 10.

![](images/cf63ef027dcd0bcbc921bf5e7e2294c6edd8a61134e067bc2637c624aa97e092.jpg)  
Figure 8: (Left) We stop optimizing at step 20 and keep the original model operating for the rest of the steps. (Right) The synthesized images with different iterations. We observe that we have better performance when setting iteration to 10.

Inaccuracy label. Fig. 9 shows that the collected man training images contain glasses, but often do not contain the glasses label.

IFID. We use the official FID code to compute the similarity/distance between two distributions of image dataset, namely the edited images and the ground truth (GT) images. This measurement assesses the overall distribution rather than a single image. In the ideal case, our goal is to suppress only the content associated with the negative target in the image while leaving the content related to the positive target unaffected. We evaluate the effectiveness of suppression by comparing the FID values of the image dataset before and after suppression. A higher FID indicates a more successful suppression effect (referred to as IFID). However, we experimentally observed that many suppression methods (e.g., Negative prompt) can inadvertently impact the positive target while suppressing the negative target. Therefore, we will use IFID as the secondary metric, and Clipscore and DetScore as the primary metrics.

DetScore. We introduce a DetScore metric. It use MMDetection (Chen et al., 2019) with GLIP (Li et al., 2022) to detect the negative prompt object from the generated image and real image (e.g., the negative prompt object ‚Äùlaptop‚Äù in prompt ‚ÄùA laptop on sofa‚Äù in Fig. 5 (left)). We refer to the prediction score as DetScore. We set the prediction score threshold to 0.7, and our method achieves the best value in quantitative evaluation in both generated and real images (see Table 1 (the fourth, seventh and tenth columns)).

Generated-image editing experiment details. The proposed method aims to focus attention on content suppression based on text embeddings, so we compare with various baselines on different types of generated images. (1) We compare our method with various baselines for generating images in the style of Van Gogh and Tyler Edlin (see Fig. 10 (Top) and Table 1 (the eleventh to the fourteenth columns)). The data related to Van Gogh and Tyler Edlin styles are sourced from the official code of ESD (Gandikota et al., 2023). This dataset comprises 50 prompts for Van Gogh style and 40 prompts for Tyler Edlin style. (2) To generate car-related images, we randomly select 50 car-related captions from COCO‚Äôs validation set as prompts for input into SD. Additionally, we use multiple seeds for the same prompts. We chose to conduct experiments using car-related images for the specific reason that all baselines can effectively erase cars from the images, whereas the removal of other content is not universally suitable across all baselines. As shown in Table 1 (the eighth to the tenth columns), our method achieves the best values on the three evaluation metrics compared with all the baselines. Quantitative comparisons to various baselines are presented in Fig. 10 (Bottom). (3) For the other generated images used in the experiments (see Fig. 6 (the fourth to the sixth columns) and Table 1 (the fifth to the seventh columns)), we randomly select 100 captions provided in the COCO‚Äôs validation set (Chen et al., 2015) as prompts, and input to the SD model.

![](images/cee0d1e595d76d46403cd0765bc00786802b94d96edc63a33793aa30170e2a09.jpg)  
Figure 9: We find the collected man training images contain glasses, but often do not contain the glasses label.

![](images/74459b858bbbf9f379a0652611303dcf5b77fa642db95c63cdbe773b8c0dde77.jpg)  
Figure 10: (Top) Comparisons with various baselines for generated images in the style of Van Gogh and Tyler Edlin. (Bottom) Comparisons with various baselines for generated car-related images.

![](images/1724e5c154ced8af6efcfb77530ea276ae534eca70e17866442f0a183e2a0107.jpg)  
Figure 11: As an example, the instruction used in Inst-Inpaint is ‚ÄùRemove the airplane at the center‚Äù, while our prompt is ‚ÄùThe airplane at the center‚Äù. GT is obtained using the image inpainting method CRFill (Zeng et al., 2021).

GQA-Inpaint dataset experiment details. Inst-Inpaint reports FID and CLIP Accuracy metrics for verification on the GQA-Inpaint dataset. FID compares the distributions of ground truth images (GT) and generated image distributions to assess the quality of images produced by a generative model. In the evaluation of Inst-Inpaint, the target image from the GQA-Inpaint dataset serves as the ground truth image when calculating FID. In Table 2 (the third column), Inst-Inpaint achieves the best FID score on the GQA-Inpaint dataset, primarily because the erasure results produced by InstInpaint (Fig. 5 (the second row, the sixth column)) closely resemble the ground truth (GT) images (Fig. 5 (the second row, the fifth column)). Inst-Inpaint introduces CLIP Accuracy as a metric to assess the accuracy of the removal operation. For CLIP Accuracy, we use the official implementation of Inst-Inpaint. Inst-Inpaint use CLIP as a zero-shot classifier to predict the semantic labels of image regions based on bounding boxes. It compare the Top1 and Top5 predictions between the source image and inpainted image, considering a success when the source image class is not in the Top1 and Top5 predictions of the inpainted image. CLIP Accuracy is defined as the percentage of success. In Table 2 (the fourth and fifth columns), ours achieves the highest CLIP Accuracy scores for both Top1 and Top5 predictions on the GQA-Inpaint dataset. This result indicates the superior accuracy of our removal process.

Inst-Inpaint requires obtaining the target image corresponding to the source image as paired data for training. It extracts segmentation masks for each object from the source image and uses them to remove objects from the source image using the inpainting method CRFill (Zeng et al., 2021). The resulting target image is used as GT (e.g., Fig. 11 (the second column)).

There are 18883 pairs of test data in the GQA-Inpaint dataset, including source image, target image, and prompt. Inst-Inpaint attempts to remove objects from the source image based on the provided prompt as an instruction (e.g., ‚ÄùRemove the airplane at the center‚Äù in Fig. 11 (the third column)). We suppress the noun immediately following ‚Äùremove‚Äù in the instruction (e.g., ‚Äùairplane‚Äù) and use the remaining part, deleting the word ‚Äùremove‚Äù at the beginning of the instruction to form our input prompt (e.g., ‚ÄùThe airplane at the center‚Äù in Fig. 11 (the fourth column)).

Baseline Implementations. For the comparisons in section 4, we use the official implementation of ESD (Gandikota et al., 2023) 9, Concept-ablation (Kumari et al., 2023) 10, Forget-Me-Not (Zhang et al., 2023) 11, Inst-Inpaint (Yildirim et al., 2023) 12 and SEGA (Brack et al., 2023) 13. We use P2P (Hertz et al., 2022) 14 with Attention Re-weighting to weaken the extent of content in the resulting images.

Failure cases. Fig. 12 shows some failure cases.

A hamburger and fries A giraffe standing next to a bamboo building ÂåÖÁáï Êµ∑ SD Ours SD Ours

B APPENDIX: EQ. 2 IN SOFT-WEIGHTED REGULARIZATION.

We take inspiration from WNNM, a method used for image denoising tasks, which demonstrates that singular values have a clear physical meaning and should be treated differently. WNNM considers that the noise in the image mainly resides in the bottom- $K$ singular values. Each singular value $\sigma$ of the image patch can be updated using the formula ‚àí (œÉŒª+œµ) and set to 0 when the updated singular value becomes less than 0. The weight $\frac { \lambda } { ( \sigma + \epsilon ) }$ is introduced to ensure that components corresponding to smaller singular values undergo more shrinkage, where $\lambda$ is a positive constant used to scale the singular values, and $\epsilon$ is a small positive constant used to avoid division by zero. In this paper, based on our observation, the top- $K$ singular values in the constructed negative target embedding matrix $\pmb { \chi } = [ \pmb { c } ^ { N E } , \pmb { c } _ { 0 } ^ { E O T } , \cdot \cdot \cdot , \pmb { c } _ { N - | \pmb { p } | - 2 } ^ { \dot { E O T } } ] ^ { * }$ mainly resides the content in the expected suppressed embedding $c ^ { N E }$ . Therefore, we utilize the formula $e ^ { - \sigma } * \sigma$ to ensure that the components corresponding to larger singular values undergo more shrinkage.

# C APPENDIX: ALGORITHM DETAIL OF GENERATED IMAGE.

<html><body><table><tr><td colspan="2">Algorithm2: Our algorithm</td></tr><tr><td colspan="2">Require: A text embeddings c= I(p) and noise vector ZT. Output: Edited image I.</td></tr><tr><td colspan="2">‚Üê SWR(c) (Eq.2); //Soft-weighted Regularization fort=T,,T‚àí1...,1do ct=c; // Inference-time text embedding optimization forite=O,...,Ite-1do ‚Üê‚ààŒ∏(2t,t,cÔºâÔºõ APE ANE</td></tr></table></body></html>

# D APPENDIX: ABLATION ANALYSIS

Verification alignment loss. As shown in Fig. 13, $\mathcal { L } _ { p l }$ can mainly hold regions that we do not want to suppress. In addition, we employ the SSIM metric to assess the influence of the $\mathcal { L } _ { p l }$ . Increasing $\mathcal { L } _ { p l }$ raised SSIM from 0.407 $( \mathrm { { S W R } } { + \mathcal { L } _ { n l } } )$ to 0.552 $( \mathrm { S W R } { + } \mathcal { L } _ { n l } { + } \mathcal { L } _ { p l } )$ , indicating that $\mathcal { L } _ { p l }$ can help preserve the rest of the regions. $S \mathrm { W } \mathrm { R } { + } \mathcal { L } _ { n l }$ , while capable of removing objects (DetScore=0.0692), tends to change the original image structure and style $( \mathbf { I F I D } { = } 2 4 2 . 3 )$ .

![](images/3c72cfd3d3529b7876a87e59907d3310daae3e00a3404032e233a0bd3ab4a101.jpg)  
Figure 13: The regions that are not expected to be suppress are structurally altered without $\mathcal { L } _ { p l }$ (third row). Our method removes the subject while mainly preserving the rest of the regions (fourth row).

Variant of soft-weighted regularization. We also explore another way to regulate the target text embedding. We directly zero out the Top-K singular values of $\pmb { \Sigma }$ ( here, $\pmb { \Sigma } = d i a g ( \sigma _ { 0 } , \sigma _ { 1 } , \cdot \cdot \cdot , \sigma _ { n _ { 0 } } )$ , $\boldsymbol { x } = \boldsymbol { U } \boldsymbol { \Sigma } \boldsymbol { V } ^ { T }$ , $\pmb { \chi } = [ \pmb { c } ^ { N E } , \pmb { c } _ { 0 } ^ { E O T } , \cdot \cdot \cdot , \pmb { c } _ { N - | p | - 2 } ^ { E O T } ]$ cENOTp 2] ), and reconstruct œáÀÜ, which is fed into SD model to generate image. Although directly zeroing out Top-K contributes to suppress the generation from the input prompt, it suffers from unexpected results (Fig. 14 (the third to the fifth columns)).

Attention map to zero. Recent work Hertz et al. (2022); Chefer et al. (2023); Parmar et al. (2023) explore the attention map to conduct varying tasks. In this paper, we also zero out the attention map which is corresponding to the target prompt, which is defined as attn2zero. As shown in Fig. 15, attn2zero method fails to suppress the target prompt in output images.

Analysis of our method in long sentences. We use the object detection method to investigate the behavior of glasses when zeroing out both ‚Äùglasses‚Äù and [EOT] embeddings in long sentences. We first randomly generate 1000 images using SD with the prompt $\pmb { p } ^ { s r c }$ ‚ÄùA man without glasses‚Äù while generating a version that zeros out both ‚Äùglasses‚Äù and [EOT] embeddings. We use MMDetection with GLIP and the prompt ‚Äùglasses‚Äù to detect the probability of glasses being present in the generated images and obtain the prediction score for ‚Äùglasses‚Äù. The average prediction scores of MMDetection of the two versions above-mentioned on 1000 images are 0.819 and 0.084 (see Table 4 (third row, first and second column)), respectively, which proves that when using prompt $\pmb { p } ^ { s r c }$ , ‚ÄùA man without glasses‚Äù, zeroing out the text embeddings of both ‚Äùglasses‚Äù and [EOT] results in the disappearance of ‚Äùglasses‚Äù in almost all generated images. It should be noted that the prediction score of MMDetection does not indicate that $8 1 . 9 \%$ of the 1000 images contain glasses. Instead, it represents the probability that the image is detected as containing glasses.

![](images/59de513b32d494ef24e6799bcb58aed4753e4e7ffc2cb13959b4b67faef52581.jpg)  
Figure 14: Variant of soft-weighted regularization. We zero out the Top-K singular values of $\pmb { \Sigma }$ ( $\pmb { \Sigma } = d i a g ( \sigma _ { 0 } , \sigma _ { 1 } , \cdot \cdot \cdot , \sigma _ { n _ { 0 } } )$ ). We experimentally observe that naively zeroing out the singular values suppresses the target prompt, but in some cases it leads to unwanted changes and expected results (the third to fifth columns).

![](images/d30666f8be968ffba6b637c082e3b186915873fa95ef2f9697c6494584a961ec.jpg)  
Figure 15: We set the attention map of the suppressed subject (e.g. glasses) to 0. We find it fail to remove this subject (third column). Ours successfully remove the subject (second column).

To investigate the behavior of glasses with long sentences, we use ChatGPT to generate description words of lengths 8, 16, and 32 after prompt $\pmb { p } ^ { s r c }$ to form new prompts denoted as $\mathbf { \nabla } p ^ { s r c + 8 w \bar { s } }$ , $p ^ { s r c + 1 6 w s }$ , and $p ^ { s r c + 3 2 w s }$ , respectively. As shown in Table 4, when zeroing out both ‚Äùglasses‚Äù and [EOT] embeddings, long sentences are harder to drop glasses than short sentences. This is due to the fact that other embeddings, except ‚Äùglasses‚Äù and [EOT], contain more glasses information compared to short sentences. However, we observe that zeroing out both ‚Äùglasses‚Äù and [EOT] embeddings works when most of the words in the prompt correspond to objects in the image, even when the sentence is long. (e.g. ‚ÄùA man with a beard wearing glasses and a hat in blue shirt‚Äù) Therefore, our method requires a concise prompt that mainly describes the object, avoiding lengthy abstract descriptions.

Table 4: The average prediction score of MMDetection with GLIP using prompt ‚Äùglasses‚Äù.   

<html><body><table><tr><td rowspan="2">Method</td><td>SD</td><td colspan="4">Zerd [EOr emh adings"</td></tr><tr><td>psrc</td><td>psrc</td><td>psrc+8ws</td><td>psrc+16ws</td><td>psrc+32ws</td></tr><tr><td>DetScore‚Üì</td><td>0.819</td><td>0.084</td><td>0.393</td><td>0.455</td><td>0.427</td></tr></table></body></html>

Different suppression levels for soft-weighted regularization. We observe that the disappearance of the negative target (e.g., glasses in Fig. 2a (the sixth and seventh columns)) occurs when the negative target information diminishes to a certain level. We perform an analysis experiment to validate this conclusion. For example, we use $\gamma$ to control the suppression levels in soft-weighted regularization using $\hat { \sigma } = e ^ { - \gamma \sigma } * \sigma$ (in Eq. 2). When $\gamma = 0$ , then $\hat { \sigma } = \sigma$ , there is no change in singular values. When $\gamma = 1$ , then $\hat { \sigma } = e ^ { - \sigma } * \sigma$ , which equals to Eq. 2 that we used. When $\gamma \to \infty$ , then $\hat { \sigma } = \operatorname* { l i m } _ { \gamma  \infty } e ^ { - \gamma \sigma } \ast \sigma = 0$ , which equals to zero out both ‚Äùglasses‚Äù and [EOT] embeddings in Fig. 2a (the sixth and seventh columns). As shown in Fig. 16, as $\gamma$ increases, the degree to which singular values are penalized gradually increases. When $\gamma$ increases to a certain level, at which the content of glasses in both the ‚Äùglasses‚Äù and [EOT] embeddings decreases to a certain level, glasses will be erased.

![](images/cc1dfc97f8514360770ec516f99bb75cbc13e2c5d378c9cc639fe323fd8b7003.jpg)  
Figure 16: Different suppression levels for soft-weighted regularization.   
Figure 17: We can suppress the content using diverse input prompts.

Robustness to diverse input prompts. As shown in Fig. 17, we showcase our robustness to diverse input prompts by effectively suppressing the content in an image using multiple prompts. It is important to emphasize that the suppressed content must be explicitly specified in the input prompt to enable our prompt-based content suppression.

bbkk A man with a beard A man in glasses A man in a beanie ‰∏≠ Real image Ours Ours Ours Ours

Evaluation the attenuation factor. We experimentally observed that employing an attenuation factor (e.g., 0.1) for the negative target embedding matrix would impact the positive target (see Fig. 20). Hence, using an attenuation factor leads to unexpected subject changes as well as changes to the target subject. This is due to the fact that the [EOT] embeddings contain significant information about the input prompt, including both the negative target and the positive target (see Sec. 3.2). Furthermore, the selection of factors needs to be carefully performed for each image to achieve satisfactory suppression results.

![](images/ebec929ab021629fdbc120971b38b13aa6383ba9d646f56ebf0921cfe35fac41.jpg)  
Figure 18: Additional reference-guided negative target generation results. Comparisons with various baselines for real image and the target prompt.

[EOT] embedding in text prompts with various lengths. We observe that the [EOT] embedding contains small yet useful semantic information, as demonstrated in Fig. 2c in our main paper. As shown in Fig. 2c, we randomly select one [EOT] embedding to replace input text embeddings. The generated images following this replacement have similar semantic information (see Fig. 2c). To further evaluate whether the [EOT] embedding contains useful semantic information in text prompts of various lengths, we replace the input text embeddings with not just one [EOT] embedding, but multiple. We use part of the [EOT] embedding when its length exceeds that of the input text embeddings (short sentence), and we copy multiple copies of the whole [EOT] embedding when its length is shorter than input text embeddings (long sentence).

In more detail, we first randomly chose 50 prompts from the prompt sets as mentioned in Sec. 4. These text prompts include various syntactical structures, such as $\mathrm { ^ { , , } A }$ living area with a television and a table‚Äù, ‚ÄùA black and white cat relaxing inside a laptop‚Äù and ‚ÄùThere is a homemade pizza on a cutting board‚Äù. We add description words with lengths 8, 16, 32 and 56 following the initial text prompt $\mathbf { p } ^ { s r c }$ to obtain a long sentence, dubbed as $\mathbf { p } ^ { s r \bar { c } + 8 w s }$ , $\mathbf { p } ^ { s r c + 1 6 w s }$ , $\mathbf { p } ^ { s r c + 3 2 w s }$ , and $\mathbf { p } ^ { s r c + 5 6 w s }$ , respectively. For instance, when $\mathbf { p } ^ { s r c }$ is $\mathrm { ^ { , , } A }$ living area with a television and a table‚Äù, $\mathbf { p } ^ { s r c + 8 w s }$ would be extended to ‚ÄùA living area with a television and a table, highly detailed and precision with extreme detail description‚Äù.

![](images/b2bfd06ae1895751ddeb27dbe14e69d5436fc9ecc33371e6fe884c0ab8c712f6.jpg)  
Figure 19: Additional latent-guided negative target generation results. Examples of our method and the baselines for generated image. We are able to suppress the target prompt, without further finetuning the SD model.

We use Clipscore to evaluate that the generated images match the given prompt. In this case, we test our model under various length prompts $( \mathbf { p } ^ { s r c } , \mathbf { p } ^ { s r c + 8 w s }$ , psrc+16ws , $\mathbf { \hat { p } } ^ { s r c + 3 2 w s }$ , and $\mathbf { p } ^ { s r c + 5 6 w s } )$ (see Table 5 (the second and third rows)). As shown in Table 5, the generated images corresponding [EOT] embedding replacement prompts also contain similar semantic information compared to the initial prompt. The degeneration of the Clipscore is small (less than 0.11), indicating that the [EOT] embedding also contains semantic information. Fig. 21 shows some more qualitative results.

Related work also consider the [EOT] embedding for other tasks. For example, P2P manipulates the [EOT] attention injection when conducing image-to-image translation. P2P (Hertz et al., 2022)

A man with a beard wearing glasses and a hat A man with a beard wearing glasses and a hat NO QNO ##N N NOS Input Attenuation factor 0.1 SVD (Ours)

Table 5: Comparison results with original tokens and their replacement version. We evaluate it with Clipscore.   

<html><body><table><tr><td>Mehod</td><td>psrc</td><td>psrc+8ws</td><td>psrc+16ws</td><td>psrc+32ws</td><td>psrc+56ws</td></tr><tr><td>SD</td><td>0.8208</td><td>0.8173</td><td>0.8162</td><td>0.8102</td><td>0.8058</td></tr><tr><td>SD w/replacement</td><td>0.7674</td><td>0.7505</td><td>0.7479</td><td>0.7264</td><td>0.7035</td></tr></table></body></html>

A black and white cat relaxing inside a laptop

![](images/7f985575ed724f471078cc732e2c56128526e2fd9815d853f4ff1326b40c950f.jpg)  
Figure 20: SWR with an attenuation factor and SVD. Note how the usage of an attenuation factor leads to undesired changes in the hat of the man (the second column).   
Figure 21: Both SD and its w/ replacement results.

swaps whole embeddings attention, including both the input text embeddings and [EOT] embedding attentions.

Taking a simple mean of the [EOT] embedding. We extract the semantic component by taking a simple Mean of the Padding Embedding ([EOT] embedding), referred as MPE. We evaluate the propsed method (i.e., SVD) and MPE. We suppress ‚Äùglasses‚Äù subject from 1000 randomly generated images with the prompt ‚ÄùA man without glasses‚Äù. Then we use MMDetection detect the probability of glasses in the generated images. Final, we report the prediction score (DetScore).

As reported in Table 6 (the third and fourth columns), we have 0.1065 MMDetection score, while MPE is 0.6266. This finding suggests that simply averaging the [EOT] embedding often fails to extract the main semantic component. Furthermore, we further zero the ‚Äôglasses‚Äô token embedding as well as MPE, it still struggles to extract ‚Äôglasses‚Äô information (0.4892 MMDetection). Fig. 22 qualitatively shows more results.

Inference-time optimization with value regulation. We propose inference-time embedding optimization to further suppress the negative target generation and encourage the positive target content, following soft-weighted regularization. This optimization method involves updating the whole text embedding, which is then transferred to both the key and value components in the cross-attention layer. Therefore, our method implicitly changes the value component in the cross-attention layer.

Table 6: Comparison between ours and MPE. We report Clipscore.   

<html><body><table><tr><td>Mehod</td><td>SD</td><td>Ours</td><td>MPE</td><td>MPE + zeroing embedding</td></tr><tr><td>DetScore ‚Üì</td><td>0.8052</td><td>0.1065</td><td>0.6266</td><td>0.4892</td></tr></table></body></html>

![](images/e3d15d42bcb287ed10c83b05a48fff63b9bd202856889c4b540794c22c02cf6d.jpg)  
Figure 22: The visualization and DetScore when using a mean of the [EOT] embedding.

Furthermore, similar to the proposed two attention losses, we attempt to use two value losses to regulate the value component in the cross-attention layer:

$$
\begin{array} { r l } & { \mathcal { L } _ { v l } = \lambda _ { p l } \mathcal { L } _ { p l } + \lambda _ { n l } \mathcal { L } _ { n l } , } \\ & { \mathcal { L } _ { p l } = \left. \hat { V } _ { t } ^ { P E } - V _ { t } ^ { P E } \right. ^ { 2 } , } \\ & { \mathcal { L } _ { n l } = - \left. \hat { V } _ { t } ^ { N E } - V _ { t } ^ { N E } \right. ^ { 2 } , } \end{array}
$$

where hyper-parameters $\lambda _ { p l }$ and $\lambda _ { n l }$ are used to balance the effects of preservation and suppression of the value. When utilizing this value loss, we find that it is hard to generate high-quality images images (Fig. 23 (the third and sixth columns)). This result indicates that directly optimizing the value embedding does not work. The potential reason is that it also influences positive target, since each token embedding contains other token embedding information after CliptextEncoder.

![](images/b67795d2fbea647ef8ff4ded4b03bb55c5e8801cb7f141c572e93c3bf6518be9.jpg)  
Figure 23: The results for generated image (left) and real image (right) of attention loss and value loss in the inference-time embedding optimization.

E APPENDIX: ADDITIONAL RESULTS

User study. The study participants were volunteers from our college. The questionnaire consisted of 20 questions, each presenting the original image generated by SD, as well as the results of various baselines and our method. Users are tasked with selecting an image in which the target subject (i.e., a car) is more accurately suppressed compared to the original image. Each question in the questionnaire presents eight options, including baselines (Negative prompt, P2P, ESD, Conceptablation, Forget-Me-Not, Inst-Inpaint and SEGA) and our method, from which users were instructed to choose one. A total of 20 users participated, resulting in a combined total of 400 samples (20 questions $\times ~ 1$ option $\times ~ 2 0$ users), with 159 samples $( 3 9 . 7 5 \% )$ favoring our method (see Fig. 5 (Right)). In the results of the user study, the values for Ours, Negative Prompt, P2P, ESD, ConceptAblation, Forget-Me-Not, Inst-Inpaint, and SEGA are 0.3975, 0.0475, 0.03, 0.1625, 0.0525, 0.01, 0.285, and 0.015, respectively.

Additional results in our approach method. Fig. 18 shows additional real-image editing results, and Fig. 19 shows additional generated-image editing results. It should be noted that the generated images, as shown in Fig. 19 (the first to fourth columns. i.e., ‚ÄùGirl without earring‚Äù, ‚ÄùWoman without mask‚Äù, ‚ÄùGirl not wearing jewelry‚Äù and ‚ÄùA car without flowers‚Äù.), are not used for quantitative evaluation metrics in Table 1 (the fifth to the seventh columns), as the occasional failure of Clipscore (Hessel et al., 2021) to recognize negative words.

Real image results in mask-based methods. Mask-based removal methods work well for isolated objects. However, they tend to fail for objects that are closely related to their surroundings. Compared to mask-based methods, our prompt-based method can automatically complete regions of removed content based on surrounding content and works equally well when removed content is closely related to surrounding content. For example, in Fig. 24, the prompt ‚ÄùA man with a beard wearing glasses and a hat in blue shirt‚Äù and the corresponding input image show that ‚Äùbeard‚Äù, ‚Äùglasses‚Äù, and ‚Äùhat‚Äù are closely related to the man (Left). Our method can successfully remove ‚Äùbeard‚Äù, ‚Äùglasses‚Äù, and ‚Äùhat‚Äù, and fill in the removed area based on the context of the ‚Äùman‚Äù (Meddle), while the mask-based removal method appears very aggressive (Right).

Ê¨¢ËøéÊîπÊØîÂø´Âø´ Real image mask-based method

Real image results in various inversion methods. Our method can combine various real image inversion techniques, including Null-text, Textual inversion mentioned in the Appendix (Textual inversion with a pivot.) of Null-text, StyleDiffusion (Li et al., 2023a), NPI (Miyake et al., 2023) and ProxNPI (Han et al., 2023) (see Fig. 25).

![](images/f140991aa7ed2c7c97bbe991f90cca5d3176cb766cd95b069bc8176a02e5b0a3.jpg)  
Figure 24: Ours can successfully remove ‚Äùbeard‚Äù, ‚Äùglasses‚Äù, and ‚Äùhat‚Äù and fill in the removed area based on the context of the ‚Äùman‚Äù (Meddle), while the mask-based method (e.g., PlaygroundAI) fails (Right). The method reliant on masking necessitates the provision of user-specified masks that define the erased areas during the inference process.   
Figure 25: Our method can combine various real image inversion techniques.

Implementation on DeepFloyd-IF diffusion model. We use Deepfloyd-IF based on the T5 transformer to extract text embeddings using the prompt ‚Äùa man without glasses‚Äù for generation. The generated output still includes the subject with ‚Äùglasses‚Äù (see Fig. 26 (Up)), although the T5 text encoder used in Deepfloyd-IF has a larger number of parameters compared to the CLIP text encoder used in SD (T5: 4762.31M vs. CLIP: 123.06M). Our method also works very well on DeepFloyd-IF diffusion model (see Fig. 26 (Bottom)).

![](images/f20681321e6d96cc44e2a1eea4ed805a7464e2f4e765a94733b0891e379ed765.jpg)  
Figure 26: (Up) Results from DeepFloyd-IF still generate the man wearing ‚Äùglasses‚Äù. (Bottom) Implementation of our method on DeepFloyd-IF.

‚ÄùA man without glasses‚Äù results on other diffusion models. When we use other diffusion models for image generation with the prompt ‚ÄùA man without glasses‚Äù as input, the generated images still show the presence of ‚Äùglasses‚Äù (see Fig. 27 (Top)). Our method can also be implemented in other versions of the StableDiffusion model, including StableDiffusion 1.5 15 and StableDiffusion 2.1 16 (see Fig. 27 (Bottom)).

![](images/f87ac05d6234b51f0aa0c8f8df1d9fe25c63b958b8715b0f7fde41c3d8ded477.jpg)  
Figure 27: (Top) Results from StableDiffusion 1.5, StableDiffusion 2.1, Ideogram and Midjourney still generate the man wearing ‚Äùglasses‚Äù. (Bottom) Our method‚Äôs implementation on StableDiffusion 1.5 and StableDiffusion 2.1.

# F APPENDIX: ADDITIONAL APPLICATIONS

Additional cracks removal and rain removal results. As shown in Fig. 28, we present additional results for both cracks removal and rain removal. (Up) Additional results for cracks removal. (Middle) We demonstrate additional results for the synthetic rainy image. (Down) Additionally, we also demonstrate additional results for the real-world rainy image.

Attend-and-Excite similar results (Generating subjects for generated image). Attend-andExcitet (Chefer et al., 2023) find that the SD model sometimes encounters failure in generating one or more subjects from the input prompt (see Fig. 29 (the first, third, and fifth columns)). They refine the cross-attention map to attend to subject tokens and excite activations. The Eq. 2 used in soft-weighted regularization utilizes the weight $e ^ { - \sigma }$ to ensure that the components corresponding to larger singular values undergo more shrinkage, as we assume that the main singular values are corresponding to the suppressed information. We make a simple modification to the weight $e ^ { - \sigma }$ in

![](images/cc4f8c94d8636c2876db430e11a0e6ac66752ef493a65dd242b9d7a6a06652a4.jpg)  
Figure 28: (Top) Cracks removal results. (Middle) Rain removal for synthetic rainy image. (Bottom) Rain removal for real-world rainy image.

Eq. 2 by using $\beta \cdot e ^ { \alpha \sigma }$ to ensure that the components corresponding to larger singular values undergo more strengthen (i.e., $\hat { \sigma } = \beta \cdot e ^ { \alpha \sigma } * \sigma )$ , where $\beta = 1 . 2$ and $\alpha = 0 . 0 0 1$ . This straightforward modification, merely involving the update of text embeddings, addresses situations where the SD model encounters failures in generating subjects (see Fig. 29 (the second, fourth, and sixth columns)).

GLIGEN similar results (Adding subjects for real image). GLIGEN (Li et al., 2023b) can enable real image grounded inpainting, allowing users to integrate reference images into the real image. We can achieve results similar to real-image grounded inpainting using only the prompt (see Fig. 30 (the second, third, fifth, and sixth columns)). In detail, we add the prompt (blue underline) of the desired subject to the prompt describing the real image and then adopt the same strategy as in the previous subsection (Attend-and-Excite similar results).

Replacing subject in the real image with another. Subject replacement is a common task in various image editing methods Meng et al. (2021); Parmar et al. (2023); Mokady et al. (2022); Li et al. (2023a); Tumanyan et al. (2023). We can edit an image by replacing subject with another using only the prompt (see Fig. 31 (the second, fourth, and sixth columns)). We replace the text of the edited subject in the source prompt with the desired one to create the target prompt. Subsequently, we translate the real image using the target prompt into latent code. We then apply the same strategy as in the previous subsection (Attend-and-Excite similar results) to obtain the edited image. For example, we can replace the ‚Äùtoothbrush‚Äù in the ‚ÄùGirl holding toothbrush‚Äù image with the ‚Äùpen‚Äù. The DetScore with ‚Äùtoothbrush‚Äù of the source image is 0.790, and the Clipscore with ‚Äùpen‚Äù of the target image is 0.728. We find that the brace is also replaced with the pen, as the cross attention map for ‚Äùtoothbrush‚Äù couples the toothbrush and the brace.

![](images/1832d7e1ccf1fea4ce508b1e66967ad701081558d407f5601b4333520e231b18.jpg)

![](images/3986cfbfe5ecd662ee3da2193863189538413b28060213ad2cc31fadef87d18c.jpg)  
Figure 30: GLIGEN similar results (Adding subjects for real image).   
Figure 31: Replacing subject in the real image with another.